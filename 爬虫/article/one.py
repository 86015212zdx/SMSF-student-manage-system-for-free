import requests
import re
import os
import html
from datetime import datetime


def extract_og_image_url(html_text):
    """
    ç²¾å‡†æå–<meta property="og:image">æ ‡ç­¾ä¸­çš„å›¾ç‰‡URL
    :param html_text: åŒ…å«è¯¥metaæ ‡ç­¾çš„HTMLæ–‡æœ¬
    :return: æå–åˆ°çš„å›¾ç‰‡URLï¼ˆæ— åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ï¼‰
    """
    # æ ¸å¿ƒæ­£åˆ™ï¼šç²¾å‡†åŒ¹é…og:imageçš„metaæ ‡ç­¾ï¼Œæå–contenté‡Œçš„URL
    pattern = r'<meta property="og:image" content="([^"]+)"'
    match = re.search(pattern, html_text)
    if match:
        return match.group(1)
    return ""

class ArticleDataCleaner:
    def __init__(self):
        # å®šä¹‰å„ç§éœ€è¦æ¸…æ´—çš„æ¨¡å¼
        self.patterns = {
            'html_tags': r'<[^>]+>',  # HTMLæ ‡ç­¾
            'empty_strong': r'<strong>\s*</strong>',  # ç©ºçš„strongæ ‡ç­¾
            'multiple_spaces': r'\s+',  # å¤šä¸ªç©ºæ ¼
            'nbsp': r'&nbsp;',  # ä¸é—´æ–­ç©ºæ ¼
            'special_chars': r'[\xa0\u200b\u200c\u200d]',  # ç‰¹æ®Šç©ºç™½å­—ç¬¦
            'copyright_patterns': [  # ç‰ˆæƒç›¸å…³ä¿¡æ¯æ¨¡å¼
                r'Â©.*?\d{4}.*?(?:Privacy Policy|Terms of Use).*',
                r'Aeon is published by.*?charity.*?',
                r'Registered charity.*?\d+\(c\)\(\d+\).*?charity',
                r'Media Group Ltd.*?\d{4}.*?\d{4}',
            ],
            'footer_patterns': [  # é¡µè„šæ— å…³ä¿¡æ¯
                r'Privacy Policy.*?Terms of Use',
                r'All rights reserved',
                r'Published by.*?(?:association|partnership)',
            ]
        }

    def clean_html_content(self, content):
        """æ¸…æ´—HTMLå†…å®¹"""
        if not content:
            return ""

        # è§£ç HTMLå®ä½“
        content = html.unescape(content)

        # ç§»é™¤HTMLæ ‡ç­¾
        content = re.sub(self.patterns['html_tags'], '', content)

        # ç§»é™¤ç©ºçš„strongæ ‡ç­¾
        content = re.sub(self.patterns['empty_strong'], '', content)

        # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
        content = re.sub(self.patterns['special_chars'], ' ', content)

        # æ¸…ç†ä¸é—´æ–­ç©ºæ ¼
        content = re.sub(self.patterns['nbsp'], ' ', content)

        # è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
        content = re.sub(self.patterns['multiple_spaces'], ' ', content)

        # å»é™¤é¦–å°¾ç©ºç™½
        content = content.strip()

        return content

    def clean_article_data(self, title, author, date, content_list):
        """æ¸…æ´—å®Œæ•´æ–‡ç« æ•°æ®"""
        cleaned_data = {
            'title': self.clean_html_content(title) if title else "",
            'author': self.clean_html_content(author) if author else "",
            'date': self.clean_date(date) if date else "",
            'content': []
        }

        # æ¸…æ´—æ¯ä¸ªæ®µè½å¹¶è¿‡æ»¤æ— å…³å†…å®¹
        for paragraph in content_list:
            cleaned_paragraph = self.clean_html_content(paragraph)
            if (cleaned_paragraph and
                not self.is_footer_content(cleaned_paragraph) and
                len(cleaned_paragraph.strip()) > 10):  # è¿‡æ»¤è¿‡çŸ­çš„å†…å®¹
                cleaned_data['content'].append(cleaned_paragraph)

        return cleaned_data

    def is_footer_content(self, content):
        """åˆ¤æ–­æ˜¯å¦ä¸ºé¡µè„šæ— å…³å†…å®¹"""
        content_lower = content.lower().strip()

        # æ£€æŸ¥ç‰ˆæƒæ¨¡å¼
        for pattern in self.patterns['copyright_patterns']:
            if re.search(pattern, content, re.IGNORECASE):
                return True

        # æ£€æŸ¥é¡µè„šæ¨¡å¼
        for pattern in self.patterns['footer_patterns']:
            if re.search(pattern, content, re.IGNORECASE):
                return True

        # æ£€æŸ¥ç‰¹å®šå…³é”®è¯
        footer_keywords = [
            'privacy policy', 'terms of use', 'all rights reserved',
            'registered charity', 'media group', 'published by'
        ]

        keyword_count = sum(1 for keyword in footer_keywords
                          if keyword in content_lower)

        # å¦‚æœåŒ…å«å¤šä¸ªé¡µè„šå…³é”®è¯ï¼Œåˆ™è®¤ä¸ºæ˜¯é¡µè„šå†…å®¹
        return keyword_count >= 2

    def clean_date(self, date_string):
        """æ¸…æ´—å’Œæ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼"""
        if not date_string:
            return ""

        try:
            # å°è¯•è§£ææ—¥æœŸ
            date_obj = datetime.strptime(date_string.strip(), '%d %B %Y')
            return date_obj.strftime('%Y-%m-%d')
        except ValueError:
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
            return date_string.strip()

    def save_cleaned_data(self, cleaned_data, filename=None):
        """ä¿å­˜æ¸…æ´—åçš„æ•°æ®"""
        if not filename:
            # åŸºäºæ ‡é¢˜ç”Ÿæˆæ–‡ä»¶å
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', cleaned_data['title'][:50])
            filename = f"{safe_title}_{cleaned_data['date']}.txt"

        with open(filename, 'w', encoding='utf-8') as f:
            # å†™å…¥æ–‡ç« å¤´éƒ¨ä¿¡æ¯
            f.write(f"æ ‡é¢˜: {cleaned_data['title']}\n")
            f.write(f"ä½œè€…: {cleaned_data['author']}\n")
            f.write(f"æ—¥æœŸ: {cleaned_data['date']}\n")
            f.write("=" * 50 + "\n\n")

            # å†™å…¥æ­£æ–‡å†…å®¹
            for i, paragraph in enumerate(cleaned_data['content'], 1):
                f.write(f"{i}. {paragraph}\n\n")

        return filename

def sanitize_filename(filename):
    """
    æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦

    Args:
        filename (str): åŸå§‹æ–‡ä»¶å

    Returns:
        str: æ¸…ç†åçš„å®‰å…¨æ–‡ä»¶å
    """
    # ç§»é™¤æˆ–æ›¿æ¢Windowséæ³•å­—ç¬¦
    illegal_chars = '<>:"/\|?*'
    for char in illegal_chars:
        filename = filename.replace(char, '_')

    # é™åˆ¶æ–‡ä»¶åé•¿åº¦
    if len(filename) > 100:
        filename = filename[:100]

    # ç§»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹
    filename = filename.strip('. ')

    # å¦‚æœæ–‡ä»¶åä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åç§°
    if not filename:
        filename = "cover_image"

    return filename + ".jpg"


def one_main(url):
    cookies = {}

    headers = {}

    response = requests.get(
        url,
        cookies=cookies,
        headers=headers,
    )
    if response.status_code == 200:
        print("INFO--æŠ“å–æˆåŠŸ")
    tit_pattern = r"article_name=(.*?)&amp;author=(.*)&amp;date=(.*?)\""
    smal_tit = r"<meta name=\"description\" content=\"(.*?)\"/>"
    cont_p = r"<p>(.*?)</p>"

    pict_url = extract_og_image_url(response.text)
    print("INFO--è§£æåˆ°å›¾åƒåœ°å€" + pict_url)

    cleaner = ArticleDataCleaner()

    # æå–åŸºæœ¬ä¿¡æ¯
    title_match = re.findall(tit_pattern, response.text)
    meta_match = re.findall(smal_tit, response.text)
    content_matches = re.findall(cont_p, response.text)

    if title_match:
        title, author, date = title_match[0]
        print(f"æ ‡é¢˜: {cleaner.clean_html_content(title)}")
        print(f"ä½œè€…: {cleaner.clean_html_content(author)}")
        print(f"æ—¥æœŸ: {cleaner.clean_date(date)}")

    if meta_match:
        description = cleaner.clean_html_content(meta_match[0])
        print(f"æè¿°: {description}")

    cleaned_content = []
    for paragraph in content_matches:
        cleaned_para = cleaner.clean_html_content(paragraph)
        if cleaned_para:  # åªä¿ç•™éç©ºæ®µè½
            cleaned_content.append(cleaned_para)

    # ä½¿ç”¨ä¸“ä¸šæ¸…æ´—å™¨å¤„ç†å®Œæ•´æ•°æ®
    cleaned_data = cleaner.clean_article_data(
        title=title if title_match else "",
        author=author if title_match else "",
        date=date if title_match else "",
        content_list=content_matches
    )

    # ä¿å­˜æ¸…æ´—åçš„æ•°æ®
    output_file = cleaner.save_cleaned_data(cleaned_data)
    print(f"\nâœ… æ•°æ®æ¸…æ´—å®Œæˆï¼Œå·²ä¿å­˜è‡³: {output_file}")
    print(f"ğŸ“Š æ¸…æ´—ç»Ÿè®¡: åŸå§‹æ®µè½æ•° {len(content_matches)}, æ¸…æ´—åæ®µè½æ•° {len(cleaned_data['content'])}")

    print("INFO--çˆ¬å–å°é¢ä¸­")
    headers = {
        'accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
        'accept-language': 'zh-CN,zh;q=0.9',
        'cache-control': 'no-cache',
        'pragma': 'no-cache',
        'priority': 'u=1, i',
        'referer': 'https://aeon.co/',
        'sec-ch-ua': '"Not(A:Brand";v="8", "Chromium";v="144", "Google Chrome";v="144"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'image',
        'sec-fetch-mode': 'no-cors',
        'sec-fetch-site': 'cross-site',
        'sec-fetch-storage-access': 'active',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36',
    }

    params = {
        'width': '1920',
        'quality': '75',
        'format': 'auto',
    }
    response = requests.get(
        re.sub(r"\?width=1200&amp;quality=75&amp;format=jpg", "", pict_url),
        params=params,
        headers=headers,
    )
    print("INFO--ä¿å­˜å°é¢ä¸­")
    # ä½¿ç”¨æ¸…ç†åçš„å®‰å…¨æ–‡ä»¶å
    safe_filename = sanitize_filename(title_match[0][0])

    # 1. è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„æ–‡ä»¶å¤¹
    script_dir = os.path.dirname(os.path.abspath(__file__))

    # 2. ä»è„šæœ¬ç›®å½•å‡ºå‘ï¼Œæ„å»ºç›®æ ‡ç›®å½•çš„ç»å¯¹è·¯å¾„
    # è¿™é‡Œçš„ ".." è¡¨ç¤ºä»è„šæœ¬ç›®å½•ï¼ˆä¾‹å¦‚ article æ–‡ä»¶å¤¹ï¼‰å‘ä¸Šèµ°ä¸€çº§
    target_dir = os.path.join(script_dir, "..","..", "web", "static", "article_covers")

    # 3. ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(target_dir, exist_ok=True)

    safe_filename = sanitize_filename(title_match[0][0])

    file_path = os.path.join(target_dir, safe_filename)

    with open(file_path, 'wb') as f:
        f.write(response.content)
    print(f"âœ… å°é¢å·²ä¿å­˜ä¸º: {safe_filename}")
    print("âœ… æ–‡ä»¶å·²ä¿å­˜åˆ°:", os.path.abspath(file_path))
